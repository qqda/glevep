% This file was created with Citavi 6.1.0.0

@misc{Dertat.2017,
 abstract = {Overview},
 author = {Dertat, Arden},
 year = {2017},
 title = {Applied Deep Learning - Part 4: Convolutional Neural Networks},
 url = {https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2},
 urldate = {04.06.2018}
}


@article{Doi.2007,
 author = {Doi, Kunio},
 year = {2007},
 title = {Computer-aided diagnosis in medical imaging: Historical review, current status and future potential},
 pages = {198--211},
 volume = {31},
 number = {4-5},
 issn = {08956111},
 journal = {Computerized Medical Imaging and Graphics},
 doi = {10.1016/j.compmedimag.2007.02.002}
}


@misc{Doukkali.2017,
 abstract = {Convolutional Neural networks allow computers to see, in other words, Convnets are used to recognize images by transforming the original$\ldots$},
 author = {Doukkali, Firdaouss},
 year = {2017},
 title = {Convolutional Neural Networks (CNN, or ConvNets) -- Firdaouss Doukkali -- Medium},
 url = {https://medium.com/@phidaouss/convolutional-neural-networks-cnn-or-convnets-d7c688b0a207},
 urldate = {19.06.2018}
}


@article{Freer.2001,
 author = {Freer, Timothy W. and Ulissey, Michael J.},
 year = {2001},
 title = {Screening Mammography with Computer-aided Detection: Prospective Study of 12,860 Patients in a Community Breast Center},
 pages = {781--786},
 volume = {220},
 number = {3},
 issn = {0033-8419},
 journal = {Radiology},
 doi = {10.1148/radiol.2203001282}
}


@misc{Jefkine.2016,
 abstract = {Backpropagation in convolutional neural networks. A closer look at the concept of weights sharing in convolutional neural networks (CNNs) and an insight on how this affects the forward and backward propagation while computing the gradients during training.},
 author = {Jefkine},
 year = {2016},
 title = {Backpropagation In Convolutional Neural Networks},
 url = {http://www.jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/},
 urldate = {09.06.2018}
}


@misc{JiYang.2017,
 abstract = {deep-learning-nano-foundation - Udacity's Deep Learning Nano Foundation program.},
 author = {{Ji Yang}},
 year = {2017},
 title = {Kulbear, deep-learning-nano-foundation},
 url = {https://github.com/Kulbear/deep-learning-nano-foundation/wiki/ReLU-and-Softmax-Activation-Functions},
 urldate = {04.06.2018}
}


@book{Kaffka.2017,
 abstract = {{\textquotedbl}Cover{\textquotedbl} -- {\textquotedbl}Titel{\textquotedbl} -- {\textquotedbl}Impressum{\textquotedbl} -- {\textquotedbl}Inhaltsverzeichnis{\textquotedbl} -- {\textquotedbl}Prolog{\textquotedbl} -- {\textquotedbl}Einleitung{\textquotedbl} -- {\textquotedbl}Kapitel 1: Neuronale Netze und das Gehirn{\textquotedbl} -- {\textquotedbl}1.1 Was sind Neuronale Netze?{\textquotedbl} -- {\textquotedbl}1.2 Das Gehirn als Vorbild{\textquotedbl} -- {\textquotedbl}1.2.1 Organisation und Physiologie des Gehirns{\textquotedbl} -- {\textquotedbl}1.2.2 Die biologische Nervenzelle{\textquotedbl} -- {\textquotedbl}Kapitel 2: Die Simulation des Gehirns{\textquotedbl} -- {\textquotedbl}2.1 Die ersten Versuche{\textquotedbl} -- {\textquotedbl}2.2 Die Hebbsche Formel{\textquotedbl} -- {\textquotedbl}2.3 Das Perzeptron{\textquotedbl} -- {\textquotedbl}2.3.1 Die Theorie von Rosenblatt{\textquotedbl} -- {\textquotedbl}2.3.2 Das Perzeptron auf dem Papier{\textquotedbl} -- {\textquotedbl}2.3.3 Das Programm zum Perzeptron{\textquotedbl} -- {\textquotedbl}Kapitel 3: Mustererkennung durch ein Hopfield-Netz{\textquotedbl} -- {\textquotedbl}3.1 Der Assoziativspeicher{\textquotedbl} -- {\textquotedbl}3.1.1 Das Hopfield-Netz auf dem Papier{\textquotedbl} -- {\textquotedbl}3.1.2 Mustererkennung{\textquotedbl} -- {\textquotedbl}3.1.3 Ein Beispiel für Mustererkennung{\textquotedbl} -- {\textquotedbl}Kapitel 4: Der bidirektionale Assoziativspeicher{\textquotedbl} -- {\textquotedbl}4.1 Verteilter Assoziativspeicher{\textquotedbl} -- {\textquotedbl}4.1.1 Der bidirektionale Assoziativspeicher auf dem Papier{\textquotedbl} -- {\textquotedbl}4.1.2 Mustererkennung mit dem bidirektionalen Assoziativspeicher{\textquotedbl} -- {\textquotedbl}4.1.3 Bilderkennung mit dem bidirektionalen Assoziativspeicher{\textquotedbl} -- {\textquotedbl}Kapitel 5: Das Backpropagation-Netz{\textquotedbl} -- {\textquotedbl}5.1 Netztopologie{\textquotedbl} -- {\textquotedbl}5.2 Die Transferfunktion{\textquotedbl} -- {\textquotedbl}5.3 Die Lernformel{\textquotedbl} -- {\textquotedbl}5.4 Das Backpropagation-Netz auf dem Papier{\textquotedbl} -- {\textquotedbl}Kapitel 6: Vorstellung eines Programms zum Ausführen Neuronaler Netze{\textquotedbl} -- {\textquotedbl}6.1 Allgemeine Erläuterungen des Programms NetAndDecision{\textquotedbl} -- {\textquotedbl}6.2 Verwalten der Projekte{\textquotedbl} -- {\textquotedbl}6.3 Die Gewichtsmatrix{\textquotedbl} -- {\textquotedbl}6.4 Beispiele erfassen{\textquotedbl} -- {\textquotedbl}6.5 Der Beispiel-Generator{\textquotedbl} -- {\textquotedbl}6.6 Das Training des Netzes{\textquotedbl} -- {\textquotedbl}6.7 Die Ausführung des Netzes{\textquotedbl} -- {\textquotedbl}6.8 Die Lernkurve{\textquotedbl} -- {\textquotedbl}6.9 Die Grafik{\textquotedbl} -- {\textquotedbl}Kapitel 7: Beispiele für Neuronale Netze{\textquotedbl} -- {\textquotedbl}7.1 Ermittlung der Wurfweite eines Steins{\textquotedbl} -- {\textquotedbl}7.2 Kreditvergabe Entscheidung{\textquotedbl} -- {\textquotedbl}7.3 Unterstützung der Kaufentscheidung für einen PC{\textquotedbl} -- {\textquotedbl}Kapitel 8: Regressionsanalyse mit einem Neuronalen Netz{\textquotedbl} -- {\textquotedbl}8.1 Die Chartanalyse mit einem Neuronalen Netz

{\textquotedbl}8.2 Die Regressionsanalyse{\textquotedbl} -- {\textquotedbl}8.3 Mehrdimensionale Funktionen{\textquotedbl} -- {\textquotedbl}Kapitel 9: Expertensysteme{\textquotedbl} -- {\textquotedbl}9.1 Das Erheben von Wissen{\textquotedbl} -- {\textquotedbl}9.2 Aufbau eines Expertensystems{\textquotedbl} -- {\textquotedbl}9.2.1 Wissensbasis{\textquotedbl} -- {\textquotedbl}9.2.2 Darstellungsproblematik von Wissen{\textquotedbl} -- {\textquotedbl}9.2.3 Regelbasis{\textquotedbl} -- {\textquotedbl}9.2.4 Datenbasis{\textquotedbl} -- {\textquotedbl}9.2.5 Regelinterpreter (Inferenzkomponente){\textquotedbl} -- {\textquotedbl}9.2.6 Userschnittstelle{\textquotedbl} -- {\textquotedbl}9.3 Vorstellung eines Programms zum Ausführen von Expertensystemen{\textquotedbl} -- {\textquotedbl}9.3.1 Verwalten von Projekten{\textquotedbl} -- {\textquotedbl}9.3.2 Expertensystem Analyse{\textquotedbl} -- {\textquotedbl}9.3.3 Expertensystem Daten{\textquotedbl} -- {\textquotedbl}9.3.4 Verwalten von Expertensystemen{\textquotedbl} -- {\textquotedbl}9.3.5 Verwalten von Attributen{\textquotedbl} -- {\textquotedbl}9.3.6 Verwalten von Regeln{\textquotedbl} -- {\textquotedbl}9.4 Regelbasis zur Auswahl von statistischen Prognoseverfahren{\textquotedbl} -- {\textquotedbl}9.4.1 Literaturstudium zum Erheben des Expertenwissens{\textquotedbl} -- {\textquotedbl}9.4.2 Ausführen des Expertensystems{\textquotedbl} -- {\textquotedbl}9.5 Regelbasis zur Unterstützung der Kaufentscheidung für einen PC{\textquotedbl} -- {\textquotedbl}Kapitel 10: Ein Backpropagation-Netz programmieren{\textquotedbl} -- {\textquotedbl}10.1 Erfassung von Passwort-Mustern{\textquotedbl} -- {\textquotedbl}10.2 Ein Neuronales Netz zur Erkennung von Passwort-Mustern{\textquotedbl} -- {\textquotedbl}10.3 Die Programmierung des Neuronalen Netzes{\textquotedbl} -- {\textquotedbl}10.3.1 Die Programmierung des Hauptprogramms{\textquotedbl} -- {\textquotedbl}10.3.2 Der Konstruktor der Klasse 'NeuralNetwork'{\textquotedbl} -- {\textquotedbl}10.3.3 Das Einlesen und Verarbeiten von Kommandos{\textquotedbl} -- {\textquotedbl}10.3.4 Weitere Methoden der Klasse 'NeuralNetwork'{\textquotedbl} -- {\textquotedbl}10.3.5 Das Einlesen der Beispieldaten{\textquotedbl} -- {\textquotedbl}10.4 Die Programmierung der Backpropagation-Technologie{\textquotedbl} -- {\textquotedbl}10.4.1 Das Ausführen des Neuronalen Netzes{\textquotedbl} -- {\textquotedbl}10.4.2 Das Training des Neuronalen Netzes{\textquotedbl} -- {\textquotedbl}10.4.3 Das Ausführen von außen{\textquotedbl} -- {\textquotedbl}Kapitel 11: Ausblick{\textquotedbl} -- {\textquotedbl}Anhang A: Anhang{\textquotedbl} -- {\textquotedbl}A.1 Die Beispielprogramme{\textquotedbl} -- {\textquotedbl}A.2 Installation der Beispielprogramme{\textquotedbl} -- {\textquotedbl}A.3 Die verwendete Datenbanktechnologie{\textquotedbl} -- {\textquotedbl}Anhang B: Dokumentierter Quelltext der Programme{\textquotedbl} -- {\textquotedbl}B.1 Das Programm Perzeptron{\textquotedbl} -- {\textquotedbl}B.2 Das Programm HopfieldNet

{\textquotedbl}B.3 Das Programm AssociativeMemory{\textquotedbl} -- {\textquotedbl}B.4 Das Programm AssociativeMemoryPicture},
 author = {Kaffka, Thomas},
 year = {2017},
 title = {Neuronale Netze - Grundlagen: Mit Beispielprogrammen in Java},
 url = {http://proquestcombo.safaribooksonline.com/9783958456099?uicode=suub},
 keywords = {ebook},
 address = {Frechen},
 edition = {1st ed.},
 publisher = {MITP},
 isbn = {9783958456075},
 series = {mitp Professional}
}


@article{Nakao.2018,
 abstract = {BACKGROUND

The usefulness of computer-assisted detection (CAD) for detecting cerebral aneurysms has been reported; therefore, the improved performance of CAD will help to detect cerebral aneurysms.

PURPOSE

To develop a CAD system for intracranial aneurysms on unenhanced magnetic resonance angiography (MRA) images based on a deep convolutional neural network (CNN) and a maximum intensity projection (MIP) algorithm, and to demonstrate the usefulness of the system by training and evaluating it using a large dataset.

STUDY TYPE

Retrospective study.

SUBJECTS

There were 450 cases with intracranial aneurysms. The diagnoses of brain aneurysms were made on the basis of MRA, which was performed as part of a brain screening program.

FIELD STRENGTH/SEQUENCE

Noncontrast-enhanced 3D time-of-flight (TOF) MRA on 3T MR scanners.

ASSESSMENT

In our CAD, we used a CNN classifier that predicts whether each voxel is inside or outside aneurysms by inputting MIP images generated from a volume of interest (VOI) around the voxel. The CNN was trained in advance using manually inputted labels. We evaluated our method using 450 cases with intracranial aneurysms, 300 of which were used for training, 50 for parameter tuning, and 100 for the final evaluation.

STATISTICAL TESTS

Free-response receiver operating characteristic (FROC) analysis.

RESULTS

Our CAD system detected 94.2{\%} (98/104) of aneurysms with 2.9 false positives per case (FPs/case). At a sensitivity of 70{\%}, the number of FPs/case was 0.26.

DATA CONCLUSION

We showed that the combination of a CNN and an MIP algorithm is useful for the detection of intracranial aneurysms.

LEVEL OF EVIDENCE

4 Technical Efficacy: Stage 1 J. Magn. Reson. Imaging 2018;47:948-953.},
 author = {Nakao, Takahiro and Hanaoka, Shouhei and Nomura, Yukihiro and Sato, Issei and Nemoto, Mitsutaka and Miki, Soichiro and Maeda, Eriko and Yoshikawa, Takeharu and Hayashi, Naoto and Abe, Osamu},
 year = {2018},
 title = {Deep neural network-based computer-assisted detection of cerebral aneurysms in MR angiography},
 pages = {948--953},
 volume = {47},
 number = {4},
 issn = {1053-1807},
 journal = {Journal of magnetic resonance imaging : JMRI},
 doi = {10.1002/jmri.25842}
}


@article{Takahashi.2017,
 author = {Takahashi, Ryohei and Kajikawa, Yuya},
 year = {2017},
 title = {Computer-aided diagnosis: A survey with bibliometric analysis},
 pages = {58--67},
 volume = {101},
 issn = {13865056},
 journal = {International Journal of Medical Informatics},
 doi = {10.1016/j.ijmedinf.2017.02.004}
}


@misc{.11.05.2018,
 abstract = {Course materials and notes for Stanford class CS231n: Convolutional Neural Networks for Visual Recognition.},
 author = {CS231n},
 editor = {CS231n},
 year = {11.05.2018},
 title = {CS231n Convolutional Neural Networks for Visual Recognition},
 url = {http://cs231n.github.io/},
 urldate = {22.05.2018}
}


@misc{Deshpande.27.04.2018,
 abstract = {Don't worry, it's easier than it looks},
 author = {Deshpande, Adit},
 editor = {{Adit Deshpande}},
 year = {2016},
 title = {A Beginner's Guide To Understanding Convolutional Neural Networks},
 url = {https://adeshpande3.github.io/A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks/},
 urldate = {22.05.2018}
}


@article{Kooi.2017,
 abstract = {Recent advances in machine learning yielded new techniques to train deep neural networks, which resulted in highly successful applications in many pattern recognition tasks such as object detection and speech recognition. In this paper we provide a head-to-head comparison between a state-of-the art in mammography CAD system, relying on a manually designed feature set and a Convolutional Neural Network (CNN), aiming for a system that can ultimately read mammograms independently. Both systems are trained on a large data set of around 45,000 images and results show the CNN outperforms the traditional CAD system at low sensitivity and performs comparable at high sensitivity. We subsequently investigate to what extent features such as location and patient information and commonly used manual features can still complement the network and see improvements at high specificity over the CNN especially with location and context features, which contain information not available to the CNN. Additionally, a reader study was performed, where the network was compared to certified screening radiologists on a patch level and we found no significant difference between the network and the readers.},
 author = {Kooi, Thijs and Litjens, Geert and {van Ginneken}, Bram and Gubern-M{\'e}rida, Albert and S{\'a}nchez, Clara I. and Mann, Ritse and {den Heeten}, Ard and Karssemeijer, Nico},
 year = {2017},
 title = {Large scale deep learning for computer aided detection of mammographic lesions},
 pages = {303--312},
 volume = {35},
 journal = {Medical image analysis},
 doi = {10.1016/j.media.2016.07.007}
}


@article{Gulshan.2016,
 author = {Gulshan, Varun and Peng, Lily and Coram, Marc and Stumpe, Martin C. and Wu, Derek and Narayanaswamy, Arunachalam and Venugopalan, Subhashini and Widner, Kasumi and Madams, Tom and Cuadros, Jorge and Kim, Ramasamy and Raman, Rajiv and Nelson, Philip C. and Mega, Jessica L. and Webster, Dale R.},
 year = {2016},
 title = {Development and Validation of a Deep Learning Algorithm for Detection of Diabetic Retinopathy in Retinal Fundus Photographs},
 pages = {2402},
 volume = {316},
 number = {22},
 issn = {0098-7484},
 journal = {JAMA},
 doi = {10.1001/jama.2016.17216}
}


@article{Litjens.2017,
 author = {Litjens, Geert and Kooi, Thijs and Bejnordi, Babak Ehteshami and Setio, Arnaud Arindra Adiyoso and Ciompi, Francesco and Ghafoorian, Mohsen and {van der Laak}, Jeroen A.W.M. and {van Ginneken}, Bram and S{\'a}nchez, Clara I.},
 year = {2017},
 title = {A survey on deep learning in medical image analysis},
 pages = {60--88},
 volume = {42},
 issn = {13618415},
 journal = {Medical Image Analysis},
 doi = {10.1016/j.media.2017.07.005}
}


@misc{Ronneberger.2015,
 abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
 author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
 date = {2015},
 title = {U-Net: Convolutional Networks for Biomedical Image Segmentation},
 url = {http://arxiv.org/pdf/1505.04597v1}
}


@misc{Szegedy.2014,
 abstract = {We propose a deep convolutional neural network architecture codenamed {\textquotedbl}Inception{\textquotedbl}, which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
 author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
 date = {2014},
 title = {Going Deeper with Convolutions},
 url = {http://arxiv.org/pdf/1409.4842v1}
}


@misc{Szegedy.2015,
 abstract = {Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2{\%} top-1 and 5.6{\%} top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5{\%} top-5 error on the validation set (3.6{\%} error on the test set) and 17.3{\%} top-1 error on the validation set.},
 author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
 date = {2015},
 title = {Rethinking the Inception Architecture for Computer Vision},
 url = {http://arxiv.org/pdf/1512.00567v3}
}


@article{Tajbakhsh.2016,
 abstract = {Training a deep convolutional neural network (CNN) from scratch is difficult because it requires a large amount of labeled training data and a great deal of expertise to ensure proper convergence. A promising alternative is to fine-tune a CNN that has been pre-trained using, for instance, a large set of labeled natural images. However, the substantial differences between natural and medical images may advise against such knowledge transfer. In this paper, we seek to answer the following central question in the context of medical image analysis: \emph{Can the use of pre-trained deep CNNs with sufficient fine-tuning eliminate the need for training a deep CNN from scratch?} To address this question, we considered 4 distinct medical imaging applications in 3 specialties (radiology, cardiology, and gastroenterology) involving classification, detection, and segmentation from 3 different imaging modalities, and investigated how the performance of deep CNNs trained from scratch compared with the pre-trained CNNs fine-tuned in a layer-wise manner. Our experiments consistently demonstrated that (1) the use of a pre-trained CNN with adequate fine-tuning outperformed or, in the worst case, performed as well as a CNN trained from scratch; (2) fine-tuned CNNs were more robust to the size of training sets than CNNs trained from scratch; (3) neither shallow tuning nor deep tuning was the optimal choice for a particular application; and (4) our layer-wise fine-tuning scheme could offer a practical way to reach the best performance for the application at hand based on the amount of available data.},
 author = {Tajbakhsh, Nima and Shin, Jae Y. and Gurudu, Suryakanth R. and Hurst, R. Todd and Kendall, Christopher B. and Gotway, Michael B. and Liang, Jianming},
 year = {2016},
 title = {Convolutional Neural Networks for Medical Image Analysis: Full Training  or Fine Tuning?},
 url = {http://arxiv.org/pdf/1706.00712v1},
 pages = {1299--1312},
 volume = {35},
 number = {5},
 issn = {0278-0062},
 journal = {IEEE Transactions on Medical Imaging},
 doi = {10.1109/TMI.2016.2535302}
}


@article{Miao.2016,
 author = {Miao, Shun and Wang, Z. Jane and Liao, Rui},
 year = {2016},
 title = {A CNN Regression Approach for Real-Time 2D/3D Registration},
 pages = {1352--1363},
 volume = {35},
 number = {5},
 issn = {0278-0062},
 journal = {IEEE Transactions on Medical Imaging},
 doi = {10.1109/TMI.2016.2521800}
}


